---
title: "Scaling LLM Inference with Optimized Sample Compute Allocation"
collection: publications
permalink: /publication/2024-10-22-osca
category: conferences
excerpt: 'This paper introduces OSCA, a pioneering algorithm that formulates sample distribution for LLM inference as a learning problem. By using a mixed-allocation strategy, OSCA achieves superior accuracy with significantly less computeâ€”128x less on code generation and 25x less on reasoning tasks.'
date: 2024-10-22
venue: 'Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2025'
paperurl: 'https://arxiv.org/abs/2410.22480'
citation: 'Kexun Zhang*, <strong>Shang Zhou</strong>*, Danqing Wang, William Yang Wang, Lei Li. (2025). "Scaling LLM Inference with Optimized Sample Compute Allocation." In <i>Proceedings of the 2025 Annual Conference of the North American Chapter of the Association for Computational Linguistics</i>.'
---

In this work, we address the high computational cost of LLM inference, especially for complex tasks requiring multiple samples. We propose OSCA (Optimized Sample Compute Allocation), a novel strategy that intelligently allocates computational resources across different samples. Our experiments show that OSCA sets a new state-of-the-art in inference efficiency.

[Download paper here](https://arxiv.org/abs/2410.22480)

Recommended citation:
Kexun Zhang*, <strong>Shang Zhou</strong>*, Danqing Wang, William Yang Wang, Lei Li. (2025). "Scaling LLM Inference with Optimized Sample Compute Allocation." In <i>Proceedings of the 2025 Annual Conference of the North American Chapter of the Association for Computational Linguistics</i>.