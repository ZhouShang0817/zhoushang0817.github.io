---
title: "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?"
collection: publications
permalink: /publication/2025-06-11-livecodebench-pro
category: conferences
excerpt: 'We introduce LiveCodeBench Pro, a new benchmark for evaluating Large Language Models on competitive programming problems, judged by human Olympiad medalists. Our work, co-led by a team of 20 researchers, quantifies a major performance gap: LLMs score nearly 0% on hard problems and consistently fail on tasks requiring deep observation and reasoning.'
date: 2025-06-11
venue: 'Under review at Conference on Neural Information Processing Systems (NeurIPS), 2025'
paperurl: 'https://arxiv.org/abs/2506.11928'
citation: 'Zihan Zheng*, Zerui Cheng*, Zeyu Shen*, <strong>Shang Zhou</strong>*, Kaiyuan Liu*, Hansen He*, et al. (2025). "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?." <i>arXiv preprint arXiv:2506.11928</i>.'
---

This paper details a multi-institutional research effort to rigorously benchmark the capabilities of modern LLMs against challenging International Collegiate Programming Contest (ICPC) problems. By having human experts (Olympiad medalists) judge the model outputs, we provide a more nuanced evaluation than simple pass/fail metrics.

[Download paper here](https://arxiv.org/abs/2506.11928)

Recommended citation:
Zihan Zheng*, Zerui Cheng*, Zeyu Shen*, <strong>Shang Zhou</strong>*, Kaiyuan Liu*, Hansen He*, Dongruixuan Li, Stanley Wei, Hangyi Hao, Jianzhu Yao, Peiyao Sheng, Zixuan Wang, Wenhao Chai, Aleksandra Korolova, Peter Henderson, Sanjeev Arora, Pramod Viswanath, Jingbo Shang, Saining Xie. (2025). "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?." <i>arXiv preprint arXiv:2506.11928</i>.